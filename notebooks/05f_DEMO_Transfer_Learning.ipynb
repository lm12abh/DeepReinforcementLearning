{"cells": [{"metadata": {"run_control": {"marked": true}}, "cell_type": "markdown", "source": "# Machine Learning Foundation\n\n## Course 5, Part g: Transfer Learning DEMO"}, {"metadata": {}, "cell_type": "markdown", "source": "For this exercise, we will use the well-known MNIST digit data. To illustrate the power and concept of transfer learning, we will train a CNN on just the digits 5,6,7,8,9.  Then we will train just the last layer(s) of the network on the digits 0,1,2,3,4 and see how well the features learned on 5-9 help with classifying 0-4.\n\nAdapted from https://github.com/fchollet/keras/blob/master/examples/mnist_transfer_cnn.py"}, {"metadata": {}, "cell_type": "code", "source": "!pip install keras==2.3.1", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "Collecting keras==2.3.1\n  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 377 kB 23.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: h5py in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.3.1) (2.10.0)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.3.1) (1.4.1)\nRequirement already satisfied: pyyaml in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.3.1) (5.4.1)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.3.1) (1.1.2)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.3.1) (1.19.2)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.3.1) (1.15.0)\nCollecting keras-applications>=1.0.6\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50 kB 16.9 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: keras-applications, keras\nSuccessfully installed keras-2.3.1 keras-applications-1.0.8\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import datetime\nimport keras\n#from keras.datasets import mnist\n#from keras.models import Sequential\n#from keras.layers import Dense, Dropout, Activation, Flatten\n#from keras.layers import Conv2D, MaxPooling2D\n#from keras import backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras import backend as K", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "Using TensorFlow backend.\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "code", "source": "#used to help some of the timing functions\nnow = datetime.datetime.now", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# set some parameters\nbatch_size = 128 # size of the batch of data used\nnum_classes = 5 # number of classes within the data\nepochs = 5 # number of times we rung through the data", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# set some more parameters\nimg_rows, img_cols = 28, 28 # setting the pixel number for rows and cols\nfilters = 32 # the depth of each one of our next layers in the convo neural net\npool_size = 2 # reduces the image size by mapping a patch of pixels to a single value\nkernel_size = 3 # grid size with weights overlayed onto the image", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## This just handles some variability in how the input data is loaded\n# using rgb would be (3,28,28), note ours is greyscale so only 1 and we have channel last\nif K.image_data_format() == 'channels_first':\n    input_shape = (1, img_rows, img_cols)\nelse:\n    input_shape = (img_rows, img_cols, 1)", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## To simplify things, write a function to include all the training steps\n## As input, function takes a model, training set, test set, and the number of classes\n## Inside the model object will be the state about which layers we are freezing and which we are training\n\n#train has features and y\n# x_train takes first value of tuple, reshape to same rows to our input shape (60000, 28, 28, 1) for example\n# making sure all values between 0 and 1\n\ndef train_model(model, train, test, num_classes):\n    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    x_train /= 255\n    x_test /= 255\n    print('x_train shape:', x_train.shape)\n    print(x_train.shape[0], 'train samples')\n    print(x_test.shape[0], 'test samples')\n\n    # convert class vectors to binary class matrices\n    y_train = keras.utils.to_categorical(train[1], num_classes)\n    y_test = keras.utils.to_categorical(test[1], num_classes)\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adadelta',\n                  metrics=['accuracy'])\n\n    t = now()\n    model.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              verbose=1, #verbose 0 results in steps showing training at each epoch to be removed\n              validation_data=(x_test, y_test))\n    print('Training time: %s' % (now() - t))\n\n    score = model.evaluate(x_test, y_test, verbose=0)\n    print('Test score:', score[0])\n    print('Test accuracy:', score[1])", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# create two datasets: one with digits below 5 and one with 5 and above\nx_train_lt5 = x_train[y_train < 5]\ny_train_lt5 = y_train[y_train < 5]\nx_test_lt5 = x_test[y_test < 5]\ny_test_lt5 = y_test[y_test < 5]\n\nx_train_gte5 = x_train[y_train >= 5]\ny_train_gte5 = y_train[y_train >= 5] - 5\nx_test_gte5 = x_test[y_test >= 5]\ny_test_gte5 = y_test[y_test >= 5] - 5", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# Define the \"feature\" layers.  These are the early layers that we expect will \"transfer\"\n# to a new problem.  We will freeze these layers during the fine-tuning process\n\nfeature_layers = [\n    Conv2D(filters, kernel_size,\n           padding='valid',\n           input_shape=input_shape),\n    Activation('relu'),\n    Conv2D(filters, kernel_size),\n    Activation('relu'),\n    MaxPooling2D(pool_size=pool_size),\n    Dropout(0.25),\n    Flatten(),\n]", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Define the \"classification\" layers.  These are the later layers that predict the specific classes from the features\n# learned by the feature layers.  This is the part of the model that needs to be re-trained for a new problem\n\nclassification_layers = [\n    Dense(128),\n    Activation('relu'),\n    Dropout(0.5),\n    Dense(num_classes),\n    Activation('softmax')\n]", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# We create our model by combining the two sets of layers as follows\nmodel = Sequential(feature_layers + classification_layers)", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Let's take a look\nmodel.summary()", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 26, 26, 32)        320       \n_________________________________________________________________\nactivation (Activation)      (None, 26, 26, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 24, 24, 32)        0         \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n_________________________________________________________________\ndropout (Dropout)            (None, 12, 12, 32)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 4608)              0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               589952    \n_________________________________________________________________\nactivation_2 (Activation)    (None, 128)               0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 645       \n_________________________________________________________________\nactivation_3 (Activation)    (None, 5)                 0         \n=================================================================\nTotal params: 600,165\nTrainable params: 600,165\nNon-trainable params: 0\n_________________________________________________________________\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# Now, let's train our model on the digits 5,6,7,8,9\n# using the function we developed\n\ntrain_model(model,\n            (x_train_gte5, y_train_gte5),\n            (x_test_gte5, y_test_gte5), num_classes)", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "x_train shape: (29404, 28, 28, 1)\n29404 train samples\n4861 test samples\nEpoch 1/5\n230/230 [==============================] - 52s 225ms/step - loss: 1.6278 - accuracy: 0.1698 - val_loss: 1.6020 - val_accuracy: 0.2380\nEpoch 2/5\n230/230 [==============================] - 51s 224ms/step - loss: 1.6009 - accuracy: 0.2300 - val_loss: 1.5721 - val_accuracy: 0.3769\nEpoch 3/5\n230/230 [==============================] - 51s 221ms/step - loss: 1.5761 - accuracy: 0.2860 - val_loss: 1.5418 - val_accuracy: 0.4625\nEpoch 4/5\n230/230 [==============================] - 52s 226ms/step - loss: 1.5453 - accuracy: 0.3540 - val_loss: 1.5099 - val_accuracy: 0.5445\nEpoch 5/5\n230/230 [==============================] - 52s 225ms/step - loss: 1.5205 - accuracy: 0.4007 - val_loss: 1.4750 - val_accuracy: 0.6106\nTraining time: 0:04:18.683933\nTest score: 1.4749772548675537\nTest accuracy: 0.610573947429657\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Freezing Layers\nKeras allows layers to be \"frozen\" during the training process.  That is, some layers would have their weights updated during the training process, while others would not.  This is a core part of transfer learning, the ability to train just the last one or several layers.\n\nNote also, that a lot of the training time is spent \"back-propagating\" the gradients back to the first layer.  Therefore, if we only need to compute the gradients back a small number of layers, the training time is much quicker per iteration.  This is in addition to the savings gained by being able to train on a smaller data set."}, {"metadata": {}, "cell_type": "code", "source": "# Freeze only the feature layers\nfor l in feature_layers:\n    l.trainable = False\n#so it tells keras in the CNN that it wants to freeze the layers", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Observe below the differences between the number of *total params*, *trainable params*, and *non-trainable params*."}, {"metadata": {}, "cell_type": "code", "source": "model.summary()\n# see the total # of prarams at 600k but trainable is less due to the fact we are freezing the earlier layers", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 26, 26, 32)        320       \n_________________________________________________________________\nactivation (Activation)      (None, 26, 26, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 24, 24, 32)        0         \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n_________________________________________________________________\ndropout (Dropout)            (None, 12, 12, 32)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 4608)              0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               589952    \n_________________________________________________________________\nactivation_2 (Activation)    (None, 128)               0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 645       \n_________________________________________________________________\nactivation_3 (Activation)    (None, 5)                 0         \n=================================================================\nTotal params: 600,165\nTrainable params: 590,597\nNon-trainable params: 9,568\n_________________________________________________________________\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "train_model(model,\n            (x_train_lt5, y_train_lt5),\n            (x_test_lt5, y_test_lt5), num_classes)", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "x_train shape: (30596, 28, 28, 1)\n30596 train samples\n5139 test samples\nEpoch 1/5\n240/240 [==============================] - 17s 69ms/step - loss: 1.5972 - accuracy: 0.2484 - val_loss: 1.5486 - val_accuracy: 0.3964\nEpoch 2/5\n240/240 [==============================] - 16s 68ms/step - loss: 1.5507 - accuracy: 0.3351 - val_loss: 1.4972 - val_accuracy: 0.4876\nEpoch 3/5\n240/240 [==============================] - 16s 67ms/step - loss: 1.5026 - accuracy: 0.4319 - val_loss: 1.4477 - val_accuracy: 0.6118\nEpoch 4/5\n240/240 [==============================] - 16s 66ms/step - loss: 1.4570 - accuracy: 0.5138 - val_loss: 1.3987 - val_accuracy: 0.7198\nEpoch 5/5\n240/240 [==============================] - 16s 66ms/step - loss: 1.4120 - accuracy: 0.5752 - val_loss: 1.3502 - val_accuracy: 0.7821\nTraining time: 0:01:20.969474\nTest score: 1.3502193689346313\nTest accuracy: 0.7820587754249573\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Note that after a single epoch, we are already achieving results on classifying 0-4 that are comparable to those achieved on 5-9 after 5 full epochs.  This despite the fact the we are only \"fine-tuning\" the last layer of the network, and all the early layers have never seen what the digits 0-4 look like.\n\nAlso, note that even though nearly all (590K/600K) of the *parameters* were trainable, the training time per epoch was still much reduced.  This is because the unfrozen part of the network was very shallow, making backpropagation faster. "}, {"metadata": {}, "cell_type": "markdown", "source": "## Exercise\n- Now we will write code to reverse this training process.  That is, train on the digits 0-4, then finetune only the last layers on the digits 5-9."}, {"metadata": {}, "cell_type": "code", "source": "# Create layers and define the model as above\n# so training on two separate datasets lt5 and gt 5\nfeature_layers2 = [\n    Conv2D(filters, kernel_size,\n           padding='valid',\n           input_shape=input_shape),\n    Activation('relu'),\n    Conv2D(filters, kernel_size),\n    Activation('relu'),\n    MaxPooling2D(pool_size=pool_size),\n    Dropout(0.25),\n    Flatten(),\n]\n\nclassification_layers2 = [\n    Dense(128),\n    Activation('relu'),\n    Dropout(0.5),\n    Dense(num_classes),\n    Activation('softmax')\n]\nmodel2 = Sequential(feature_layers2 + classification_layers2)\nmodel2.summary()", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 26, 26, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 24, 24, 32)        9248      \n_________________________________________________________________\nactivation_5 (Activation)    (None, 24, 24, 32)        0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 12, 12, 32)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 4608)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               589952    \n_________________________________________________________________\nactivation_6 (Activation)    (None, 128)               0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 5)                 645       \n_________________________________________________________________\nactivation_7 (Activation)    (None, 5)                 0         \n=================================================================\nTotal params: 600,165\nTrainable params: 600,165\nNon-trainable params: 0\n_________________________________________________________________\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# Now, let's train our model on the digits 0,1,2,3,4\ntrain_model(model2,\n            (x_train_lt5, y_train_lt5),\n            (x_test_lt5, y_test_lt5), num_classes)", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "x_train shape: (30596, 28, 28, 1)\n30596 train samples\n5139 test samples\nEpoch 1/5\n240/240 [==============================] - 53s 221ms/step - loss: 1.6034 - accuracy: 0.2393 - val_loss: 1.5555 - val_accuracy: 0.4695\nEpoch 2/5\n240/240 [==============================] - 52s 216ms/step - loss: 1.5527 - accuracy: 0.3558 - val_loss: 1.4994 - val_accuracy: 0.7159\nEpoch 3/5\n240/240 [==============================] - 53s 221ms/step - loss: 1.5026 - accuracy: 0.4693 - val_loss: 1.4387 - val_accuracy: 0.7935\nEpoch 4/5\n240/240 [==============================] - 54s 223ms/step - loss: 1.4452 - accuracy: 0.5648 - val_loss: 1.3692 - val_accuracy: 0.8284\nEpoch 5/5\n240/240 [==============================] - 53s 221ms/step - loss: 1.3800 - accuracy: 0.6310 - val_loss: 1.2882 - val_accuracy: 0.8579\nTraining time: 0:04:24.731901\nTest score: 1.288170337677002\nTest accuracy: 0.8579490184783936\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "#Freeze layers\nfor l in feature_layers2:\n    l.trainable = False", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model2.summary()", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 26, 26, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 24, 24, 32)        9248      \n_________________________________________________________________\nactivation_5 (Activation)    (None, 24, 24, 32)        0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 12, 12, 32)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 4608)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               589952    \n_________________________________________________________________\nactivation_6 (Activation)    (None, 128)               0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 5)                 645       \n_________________________________________________________________\nactivation_7 (Activation)    (None, 5)                 0         \n=================================================================\nTotal params: 600,165\nTrainable params: 590,597\nNon-trainable params: 9,568\n_________________________________________________________________\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "train_model(model2,\n            (x_train_gte5, y_train_gte5),\n            (x_test_gte5, y_test_gte5), num_classes)\n#by flipping what we trained first before we got slightly worse results which defines that it is an art note we could have added epoch and been faster.", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "x_train shape: (29404, 28, 28, 1)\n29404 train samples\n4861 test samples\nEpoch 1/5\n230/230 [==============================] - 16s 69ms/step - loss: 1.5869 - accuracy: 0.2892 - val_loss: 1.5374 - val_accuracy: 0.3950\nEpoch 2/5\n230/230 [==============================] - 16s 69ms/step - loss: 1.5458 - accuracy: 0.3339 - val_loss: 1.4937 - val_accuracy: 0.4384\nEpoch 3/5\n230/230 [==============================] - 16s 68ms/step - loss: 1.5062 - accuracy: 0.3844 - val_loss: 1.4510 - val_accuracy: 0.5087\nEpoch 4/5\n230/230 [==============================] - 16s 68ms/step - loss: 1.4682 - accuracy: 0.4327 - val_loss: 1.4096 - val_accuracy: 0.5986\nEpoch 5/5\n230/230 [==============================] - 16s 67ms/step - loss: 1.4280 - accuracy: 0.4910 - val_loss: 1.3691 - val_accuracy: 0.6686\nTraining time: 0:01:18.843190\nTest score: 1.3690930604934692\nTest accuracy: 0.6685867309570312\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "---\n### Machine Learning Foundation (C) 2020 IBM Corporation"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.8", "language": "python"}, "language_info": {"name": "python", "version": "3.8.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}