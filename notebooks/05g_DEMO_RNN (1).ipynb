{"cells": [{"metadata": {"run_control": {"marked": true}}, "cell_type": "markdown", "source": "# Machine Learning Foundation\n\n## Course 5, Part g: RNN DEMO"}, {"metadata": {}, "cell_type": "markdown", "source": "## Using RNNs to classify sentiment on IMDB data\nFor this exercise, we will train a \"vanilla\" RNN to predict the sentiment on IMDB reviews.  Our data consists of 25000 training sequences and 25000 test sequences.  The outcome is binary (positive/negative) and both outcomes are equally represented in both the training and the test set.\n\nKeras provides a convenient interface to load the data and immediately encode the words into integers (based on the most common words).  This will save us a lot of the drudgery that is usually involved when working with raw text.\n\nWe will walk through the preparation of the data and the building of an RNN model.  Then it will be your turn to build your own models (and prepare the data how you see fit)."}, {"metadata": {}, "cell_type": "code", "source": "#from tensorflow import keras\n#from tensorflow.keras.preprocessing import sequence\n#from tensorflow.keras.models import Sequential\n#from tensorflow.keras.layers import Dense, Embedding\n#from tensorflow.keras.layers import SimpleRNN\n#from tensorflow.keras.datasets import imdb\n#from tensorflow.keras import initializers\nimport keras\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import SimpleRNN\nfrom keras.datasets import imdb\nfrom keras import initializers\nimport tensorflow as tf", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "max_features = 20000  # This is used in loading the data, picks the most common (max_features) words\nmaxlen = 30  # maximum length of a sequence - truncate after this\nbatch_size = 32 # Thus will go through many iterations of gradient descent before an epoch", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## Load in the data.  The function automatically tokenizes the text into distinct integers\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) #Max features is just the 20,000 most common words within our dataset\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n17465344/17464789 [==============================] - 0s 0us/step\n17473536/17464789 [==============================] - 0s 0us/step\n25000 train sequences\n25000 test sequences\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# This pads (or truncates) the sequences so that they are of the maximum length\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "x_train shape: (25000, 30)\nx_test shape: (25000, 30)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "x_train[123,:]  #Here's what an example sequence looks like", "execution_count": 5, "outputs": [{"output_type": "execute_result", "execution_count": 5, "data": {"text/plain": "array([  219,   141,    35,   221,   956,    54,    13,    16,    11,\n        2714,    61,   322,   423,    12,    38,    76,    59,  1803,\n          72,     8, 10508,    23,     5,   967,    12,    38,    85,\n          62,   358,    99], dtype=int32)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Keras layers for (Vanilla) RNNs\n\nIn this exercise, we will not use pre-trained word vectors.  Rather we will learn an embedding as part of the Neural Network.  This is represented by the Embedding Layer below.\n\n### Embedding Layer\n`keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)`\n\n- This layer maps each integer into a distinct (dense) word vector of length `output_dim` i.e 50 in this case.\n- Can think of this as learning a word vector embedding \"on the fly\" rather than using an existing mapping (like GloVe). So in our case will be specific to imdb.\n- The `input_dim` should be the size of the vocabulary.\n- The `input_length` specifies the length of the sequences that the network expects. Going to keep at 30 by padding or truncating accordingly.\n\n`word_embedding_dim = 5` so input dimension is a vector of 50 numbers, one vector should be close to another if they are similar in meaning.\n\n### SimpleRNN Layer\n`keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n\n- kernel_initializer, is our initial weight values for our matrices (input). Same with recurrent_initializer, weights for state layers.\n- This is the basic RNN, where the output is also fed back as the \"hidden state\" to the next iteration.\n- The parameter `units` gives the dimensionality of the output (and therefore the hidden state).  Note that typically there will be another layer after the RNN mapping the (RNN) output to the network output.  So we should think of this value as the desired dimensionality of the hidden state and not necessarily the desired output of the network.\n- Recall that there are two sets of weights, one for the \"recurrent\" phase and the other for the \"kernel\" phase.  These can be configured separately in terms of their initialization, regularization, etc.\n\n\n\n\n"}, {"metadata": {}, "cell_type": "code", "source": "# Confirming our input shape\nx_train.shape[1:]", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "(30,)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "## Let's build a RNN\n# Note here we have changed our activation function to ReLu\n\nrnn_hidden_dim = 5\nword_embedding_dim = 50 # Take our integers and come up with an embedding where it will transfer each integer into a vector of dim 50\nmodel_rnn = Sequential()\nmodel_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence and embeds it in a 50-dimensional vector\n# Initializing weights for the first layer (kernel), and state layer (recurrent).\nmodel_rnn.add(SimpleRNN(rnn_hidden_dim,\n                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n                    recurrent_initializer=initializers.Identity(gain=1.0),\n                    activation='relu',\n                    input_shape=x_train.shape[1:]))\n# Dense layer provides us with the classification output\nmodel_rnn.add(Dense(1, activation='sigmoid'))", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## Note that most of the parameters come from the embedding layer\n# Simple rnn layer, initial matrix input to state should have a 50 input, 5 hidden, add on bias term 50x5 +5 for the bias term so 255 then we use 5x5 matrix from one layer to the next so 280.\n# Dense layer, 5 input plus the bias term.\nmodel_rnn.summary()", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, None, 50)          1000000   \n                                                                 \n simple_rnn (SimpleRNN)      (None, 5)                 280       \n                                                                 \n dense (Dense)               (None, 1)                 6         \n                                                                 \n=================================================================\nTotal params: 1,000,286\nTrainable params: 1,000,286\nNon-trainable params: 0\n_________________________________________________________________\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "rmsprop = tf.keras.optimizers.RMSprop(learning_rate = .0001)\n\n# binary_crossentropy as we are looking at a classification output\nmodel_rnn.compile(loss='binary_crossentropy',\n              optimizer=rmsprop,\n              metrics=['accuracy'])", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_rnn.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=10,\n          validation_data=(x_test, y_test))", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n782/782 [==============================] - 8s 9ms/step - loss: 0.6586 - accuracy: 0.6281 - val_loss: 0.5990 - val_accuracy: 0.6879\nEpoch 2/10\n782/782 [==============================] - 7s 9ms/step - loss: 0.5520 - accuracy: 0.7230 - val_loss: 0.5477 - val_accuracy: 0.7159\nEpoch 3/10\n782/782 [==============================] - 7s 9ms/step - loss: 0.4944 - accuracy: 0.7634 - val_loss: 0.5098 - val_accuracy: 0.7465\nEpoch 4/10\n782/782 [==============================] - 7s 9ms/step - loss: 0.4553 - accuracy: 0.7870 - val_loss: 0.4878 - val_accuracy: 0.7592\nEpoch 5/10\n782/782 [==============================] - 7s 9ms/step - loss: 0.4267 - accuracy: 0.8042 - val_loss: 0.4748 - val_accuracy: 0.7671\nEpoch 6/10\n782/782 [==============================] - 7s 9ms/step - loss: 0.4059 - accuracy: 0.8161 - val_loss: 0.4629 - val_accuracy: 0.7765\nEpoch 7/10\n782/782 [==============================] - 7s 9ms/step - loss: 0.3903 - accuracy: 0.8251 - val_loss: 0.4541 - val_accuracy: 0.7818\nEpoch 8/10\n782/782 [==============================] - 7s 9ms/step - loss: 0.3787 - accuracy: 0.8312 - val_loss: 0.4509 - val_accuracy: 0.7848\nEpoch 9/10\n782/782 [==============================] - 7s 9ms/step - loss: 0.3688 - accuracy: 0.8369 - val_loss: 0.4528 - val_accuracy: 0.7864\nEpoch 10/10\n782/782 [==============================] - 7s 9ms/step - loss: 0.3616 - accuracy: 0.8408 - val_loss: 0.4483 - val_accuracy: 0.7874\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 13, "data": {"text/plain": "<keras.callbacks.History at 0x7f0feaddf0d0>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "score, acc = model_rnn.evaluate(x_test, y_test,\n                            batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "782/782 [==============================] - 2s 2ms/step - loss: 0.4483 - accuracy: 0.7874\nTest score: 0.44827160239219666\nTest accuracy: 0.7874400019645691\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "score is the binary cross entropy loss (log loss)"}, {"metadata": {}, "cell_type": "markdown", "source": "## Exercise\n\nIn this exercise, we will illustrate:\n- Preparing the data to use sequences of length 80 rather than length 30.  Does it improve the performance?\n- Trying different values of the \"max_features\".  Does this  improve the performance?\n- Trying smaller and larger sizes of the RNN hidden dimension.  How does it affect the model performance?  How does it affect the run time?"}, {"metadata": {}, "cell_type": "code", "source": "max_features = 20000  # This is used in loading the data, picks the most common (max_features) words\nmaxlen = 80  # maximum length of a sequence - truncate after this to see difference in performance compared to above\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "rnn_hidden_dim = 5\nword_embedding_dim = 50\nmodel_rnn = Sequential()\nmodel_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence\nmodel_rnn.add(SimpleRNN(rnn_hidden_dim,\n                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n                    recurrent_initializer=initializers.Identity(gain=1.0),\n                    activation='relu',\n                    input_shape=x_train.shape[1:]))\n\nmodel_rnn.add(Dense(1, activation='sigmoid'))", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "rmsprop = tf.keras.optimizers.RMSprop(learning_rate = .0001)\n\nmodel_rnn.compile(loss='binary_crossentropy',\n              optimizer=rmsprop,\n              metrics=['accuracy'])", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_rnn.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=10,\n          validation_data=(x_test, y_test))", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n782/782 [==============================] - 15s 18ms/step - loss: 0.5934 - accuracy: 0.6782 - val_loss: 0.5073 - val_accuracy: 0.7557\nEpoch 2/10\n782/782 [==============================] - 14s 18ms/step - loss: 0.4475 - accuracy: 0.7930 - val_loss: 0.4472 - val_accuracy: 0.7877\nEpoch 3/10\n782/782 [==============================] - 15s 19ms/step - loss: 0.3791 - accuracy: 0.8331 - val_loss: 0.4071 - val_accuracy: 0.8145\nEpoch 4/10\n782/782 [==============================] - 15s 19ms/step - loss: 0.3391 - accuracy: 0.8550 - val_loss: 0.3989 - val_accuracy: 0.8202\nEpoch 5/10\n782/782 [==============================] - 13s 16ms/step - loss: 0.3138 - accuracy: 0.8684 - val_loss: 0.3885 - val_accuracy: 0.8248\nEpoch 6/10\n782/782 [==============================] - 14s 18ms/step - loss: 0.2944 - accuracy: 0.8794 - val_loss: 0.3753 - val_accuracy: 0.8315\nEpoch 7/10\n782/782 [==============================] - 13s 17ms/step - loss: 0.2787 - accuracy: 0.8858 - val_loss: 0.3855 - val_accuracy: 0.8279\nEpoch 8/10\n782/782 [==============================] - 14s 18ms/step - loss: 0.2665 - accuracy: 0.8910 - val_loss: 0.3719 - val_accuracy: 0.8364\nEpoch 9/10\n782/782 [==============================] - 13s 17ms/step - loss: 0.2570 - accuracy: 0.8946 - val_loss: 0.3683 - val_accuracy: 0.8368\nEpoch 10/10\n782/782 [==============================] - 13s 17ms/step - loss: 0.2487 - accuracy: 0.8985 - val_loss: 0.3720 - val_accuracy: 0.8374\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 18, "data": {"text/plain": "<keras.callbacks.History at 0x7f0fea5a5cd0>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "<b> Result comparison between maxlen between 30 vs 80 </b>\n- 30 has a result of 0.787\n- 80 has an accuracy of 0.837.\n\nSo a longer sentence has a better accuracy rate."}, {"metadata": {}, "cell_type": "code", "source": "max_features = 5000  # This is used in loading the data, picks the most common (max_features) words\nmaxlen = 80  # maximum length of a sequence - truncate after this\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)", "execution_count": 23, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "rnn_hidden_dim = 5\nword_embedding_dim = 20\nmodel_rnn = Sequential()\nmodel_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence\nmodel_rnn.add(SimpleRNN(rnn_hidden_dim,\n                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n                    recurrent_initializer=initializers.Identity(gain=1.0),\n                    activation='relu',\n                    input_shape=x_train.shape[1:]))\n\nmodel_rnn.add(Dense(1, activation='sigmoid'))", "execution_count": 24, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "rmsprop = tf.keras.optimizers.RMSprop(learning_rate = .0001)\n\nmodel_rnn.compile(loss='binary_crossentropy',\n              optimizer=rmsprop,\n              metrics=['accuracy'])", "execution_count": 25, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_rnn.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=10,\n          validation_data=(x_test, y_test))", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n782/782 [==============================] - 13s 15ms/step - loss: 0.6679 - accuracy: 0.5524 - val_loss: 0.6261 - val_accuracy: 0.7265\nEpoch 2/10\n782/782 [==============================] - 11s 14ms/step - loss: 0.5976 - accuracy: 0.7384 - val_loss: 0.6059 - val_accuracy: 0.7048\nEpoch 3/10\n782/782 [==============================] - 12s 15ms/step - loss: 0.5613 - accuracy: 0.7810 - val_loss: 0.5384 - val_accuracy: 0.7648\nEpoch 4/10\n782/782 [==============================] - 12s 15ms/step - loss: 0.4805 - accuracy: 0.7817 - val_loss: 0.5446 - val_accuracy: 0.7528\nEpoch 5/10\n782/782 [==============================] - 13s 17ms/step - loss: 0.4461 - accuracy: 0.7992 - val_loss: 0.5300 - val_accuracy: 0.7629\nEpoch 6/10\n782/782 [==============================] - 10s 13ms/step - loss: 0.4253 - accuracy: 0.8084 - val_loss: 0.4799 - val_accuracy: 0.7850\nEpoch 7/10\n782/782 [==============================] - 11s 14ms/step - loss: 0.4099 - accuracy: 0.8188 - val_loss: 0.4740 - val_accuracy: 0.7888\nEpoch 8/10\n782/782 [==============================] - 12s 15ms/step - loss: 0.3973 - accuracy: 0.8256 - val_loss: 0.4427 - val_accuracy: 0.7982\nEpoch 9/10\n782/782 [==============================] - 12s 15ms/step - loss: 0.3869 - accuracy: 0.8323 - val_loss: 0.4439 - val_accuracy: 0.8018\nEpoch 10/10\n782/782 [==============================] - 12s 15ms/step - loss: 0.3759 - accuracy: 0.8376 - val_loss: 0.4370 - val_accuracy: 0.8031\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 26, "data": {"text/plain": "<keras.callbacks.History at 0x7f0fe6d13fd0>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "<b> Result comparison between max number of features </b>\\\nComparing top 20,000 features vs 5,000\n- 20,000 has a result of 0.787\n- 5,000 has an accuracy of 0.803.\n\nSo in our case the top 5,000 words has a better result.\nNote rember if our accuracy is still improbing at the end of our epochs we should increase the number."}, {"metadata": {}, "cell_type": "code", "source": "# Out of curiosity, run for 10 more epochs, this will run for 10 more\nmodel_rnn.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=10,\n          validation_data=(x_test, y_test))", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "Epoch 1/10\n782/782 [==============================] - 11s 15ms/step - loss: 0.3662 - accuracy: 0.8420 - val_loss: 0.4233 - val_accuracy: 0.8091\nEpoch 2/10\n782/782 [==============================] - 11s 14ms/step - loss: 0.3584 - accuracy: 0.8471 - val_loss: 0.4177 - val_accuracy: 0.8117\nEpoch 3/10\n782/782 [==============================] - 11s 14ms/step - loss: 0.3518 - accuracy: 0.8498 - val_loss: 0.4285 - val_accuracy: 0.8082\nEpoch 4/10\n782/782 [==============================] - 11s 15ms/step - loss: 0.3454 - accuracy: 0.8530 - val_loss: 0.4109 - val_accuracy: 0.8142\nEpoch 5/10\n782/782 [==============================] - 11s 14ms/step - loss: 0.3393 - accuracy: 0.8555 - val_loss: 0.4046 - val_accuracy: 0.8167\nEpoch 6/10\n782/782 [==============================] - 12s 15ms/step - loss: 0.3330 - accuracy: 0.8577 - val_loss: 0.4024 - val_accuracy: 0.8170\nEpoch 7/10\n782/782 [==============================] - 11s 14ms/step - loss: 0.3274 - accuracy: 0.8607 - val_loss: 0.3997 - val_accuracy: 0.8162\nEpoch 8/10\n782/782 [==============================] - 11s 14ms/step - loss: 0.3225 - accuracy: 0.8619 - val_loss: 0.3937 - val_accuracy: 0.8211\nEpoch 9/10\n782/782 [==============================] - 12s 15ms/step - loss: 0.3171 - accuracy: 0.8665 - val_loss: 0.4079 - val_accuracy: 0.8140\nEpoch 10/10\n782/782 [==============================] - 12s 15ms/step - loss: 0.3125 - accuracy: 0.8674 - val_loss: 0.3975 - val_accuracy: 0.8184\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 28, "data": {"text/plain": "<keras.callbacks.History at 0x7f0fe09c67c0>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Able to get a result of 0.82 which we were able to get using 10 epochs for a max len of 80 and 20,000 max features."}, {"metadata": {}, "cell_type": "markdown", "source": "---\n### Machine Learning Foundation (C) 2020 IBM Corporation"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.9", "language": "python"}, "language_info": {"name": "python", "version": "3.9.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}