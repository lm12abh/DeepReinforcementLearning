{"cells": [{"metadata": {"run_control": {"marked": true}}, "cell_type": "markdown", "source": "# Machine Learning Foundation\n\n## Course 5, Part e: CNN DEMO"}, {"metadata": {}, "cell_type": "markdown", "source": "## Building a CNN to classify images in the CIFAR-10 Dataset\n\nWe will work with the CIFAR-10 Dataset.  This is a well-known dataset for image classification, which consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n\nThe 10 classes are:\n\n<ol start=\"0\">\n<li> airplane\n<li>  automobile\n<li> bird\n<li>  cat\n<li> deer\n<li> dog\n<li>  frog\n<li>  horse\n<li>  ship\n<li>  truck\n</ol>\n\nFor details about CIFAR-10 see:\nhttps://www.cs.toronto.edu/~kriz/cifar.html\n\nFor a compilation of published performance results on CIFAR 10, see:\nhttp://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n\n---\n\n### Building Convolutional Neural Nets\n\nIn this exercise we will build and train our first convolutional neural networks.  In the first part, we walk through the different layers and how they are configured.  In the second part, you will build your own model, train it, and compare the performance."}, {"metadata": {"collapsed": true}, "cell_type": "code", "source": "!pip install keras==2.3.1", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Collecting keras==2.3.1\n  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 377 kB 11.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.3.1) (5.4.1)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.3.1) (1.15.0)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.3.1) (1.1.2)\nCollecting keras-applications>=1.0.6\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50 kB 14.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.3.1) (1.19.2)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.3.1) (1.4.1)\nRequirement already satisfied: h5py in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.3.1) (2.10.0)\nInstalling collected packages: keras-applications, keras\nSuccessfully installed keras-2.3.1 keras-applications-1.0.8\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import keras\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\n#from keras.datasets import cifar10\n#from keras.preprocessing.image import ImageDataGenerator\n#from keras.models import Sequential\n#from keras.layers import Dense, Dropout, Activation, Flatten\n#from keras.layers import Conv2D, MaxPooling2D\nimport matplotlib.pyplot as plt", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "Using TensorFlow backend.\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "code", "source": "# The data, shuffled and split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 5s 0us/step\nx_train shape: (50000, 32, 32, 3)\n50000 train samples\n10000 test samples\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "## Each image is a 32 x 32 x 3 numpy array 32x32 for our height and width of pixels\n# 3 for our RedGreenBlue layers\nx_train[444].shape", "execution_count": 4, "outputs": [{"output_type": "execute_result", "execution_count": 4, "data": {"text/plain": "(32, 32, 3)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "## Let's look at one of the images\n\nprint(y_train[444]) #label\nplt.imshow(x_train[444]); #image", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "[9]\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<Figure size 432x288 with 1 Axes>", "image/png": "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\n"}, "metadata": {"needs_background": "light"}}]}, {"metadata": {}, "cell_type": "code", "source": "num_classes = 10\n#turning classes to categorical variables\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# now instead of classes described by an integer between 0-9 we have a vector \n# with a 1 in the (Pythonic) 9th position\ny_train[444]", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# As before, let's make everything float and scale\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255 #scaling them down between 0 and 1\nx_test /= 255", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Keras Layers for CNNs\n- Previously we built Neural Networks using primarily the Dense, Activation and Dropout Layers.\n\n- Here we will describe how to use some of the CNN-specific layers provided by Keras\n\n### Conv2D\n\n```python\nkeras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n```\n\nA few parameters explained:\n- `filters`: the number of filter used per location.  In other words, the depth of the output.\n- `kernel_size`: an (x,y) tuple giving the height and width of the kernel to be used\n- `strides`: and (x,y) tuple giving the stride in each dimension. How we move from left to right and up to down. Default is `(1,1)`\n- `input_shape`: required only for the first layer\n\nNote, the size of the output will be determined by the kernel_size, strides. Also here we have no padding so we will stop at the end of the image.\n\n### MaxPooling2D\n`keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`\n\n- `pool_size`: the (x,y) size of the grid to be pooled.\n- `strides`: Assumed to be the `pool_size` unless otherwise specified\n\n### Flatten\nTurns its input into a one-dimensional vector (per instance).  Usually used when transitioning between convolutional layers and fully connected layers.\n\n---\n\n## First CNN\nBelow we will build our first CNN.  For demonstration purposes (so that it will train quickly) it is not very deep and has relatively few parameters.  We use strides of 2 in the first two convolutional layers which quickly reduces the dimensions of the output.  After a MaxPooling layer, we flatten, and then have a single fully connected layer before our final classification layer."}, {"metadata": {}, "cell_type": "code", "source": "# Let's build a CNN using Keras' Sequential capabilities\n#initialize\nmodel_1 = Sequential()\n\n## 5x5 convolution with 2x2 stride and 32 filters\nmodel_1.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel_1.add(Activation('relu'))\n\n## Another 5x5 convolution with 2x2 stride and 32 filters\nmodel_1.add(Conv2D(32, (5, 5), strides = (2,2)))\nmodel_1.add(Activation('relu'))\n\n## 2x2 max pooling reduces to 3 x 3 x 32\nmodel_1.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_1.add(Dropout(0.25))\n\n## Flatten turns 3x32x32 into 288x1\nmodel_1.add(Flatten())\nmodel_1.add(Dense(512))\nmodel_1.add(Activation('relu')) # for non linearality\nmodel_1.add(Dropout(0.5)) # extra regularization\nmodel_1.add(Dense(num_classes)) # so output equal to the number of classes we have\nmodel_1.add(Activation('softmax')) # as we do when we are trying to predict multiple categories \n\nmodel_1.summary()", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 16, 16, 32)        2432      \n_________________________________________________________________\nactivation (Activation)      (None, 16, 16, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 6, 6, 32)          25632     \n_________________________________________________________________\nactivation_1 (Activation)    (None, 6, 6, 32)          0         \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 3, 3, 32)          0         \n_________________________________________________________________\ndropout (Dropout)            (None, 3, 3, 32)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 288)               0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               147968    \n_________________________________________________________________\nactivation_2 (Activation)    (None, 512)               0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                5130      \n_________________________________________________________________\nactivation_3 (Activation)    (None, 10)                0         \n=================================================================\nTotal params: 181,162\nTrainable params: 181,162\nNon-trainable params: 0\n_________________________________________________________________\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "We still have 181K parameters, even though this is a \"small\" model.\n"}, {"metadata": {}, "cell_type": "code", "source": "from tensorflow.keras.optimizers import RMSprop", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "batch_size = 32\n\n# initiate RMSprop optimizer\nopt = RMSprop(lr=0.0005, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel_1.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nmodel_1.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=15,\n              validation_data=(x_test, y_test),\n              shuffle=True)", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "Epoch 1/15\n1563/1563 [==============================] - 90s 57ms/step - loss: 1.9235 - accuracy: 0.2890 - val_loss: 1.4904 - val_accuracy: 0.4689\nEpoch 2/15\n1563/1563 [==============================] - 88s 56ms/step - loss: 1.4982 - accuracy: 0.4564 - val_loss: 1.3261 - val_accuracy: 0.5182\nEpoch 3/15\n1563/1563 [==============================] - 88s 56ms/step - loss: 1.3775 - accuracy: 0.5066 - val_loss: 1.2757 - val_accuracy: 0.5435\nEpoch 4/15\n1563/1563 [==============================] - 88s 56ms/step - loss: 1.2811 - accuracy: 0.5459 - val_loss: 1.2064 - val_accuracy: 0.5671\nEpoch 5/15\n1563/1563 [==============================] - 87s 56ms/step - loss: 1.2511 - accuracy: 0.5580 - val_loss: 1.1796 - val_accuracy: 0.5879\nEpoch 6/15\n1563/1563 [==============================] - 88s 56ms/step - loss: 1.2049 - accuracy: 0.5762 - val_loss: 1.1117 - val_accuracy: 0.6056\nEpoch 7/15\n1563/1563 [==============================] - 88s 56ms/step - loss: 1.1741 - accuracy: 0.5846 - val_loss: 1.2150 - val_accuracy: 0.5720\nEpoch 8/15\n1563/1563 [==============================] - 88s 56ms/step - loss: 1.1584 - accuracy: 0.5892 - val_loss: 1.1727 - val_accuracy: 0.5954\nEpoch 9/15\n1563/1563 [==============================] - 87s 56ms/step - loss: 1.1486 - accuracy: 0.5967 - val_loss: 1.0745 - val_accuracy: 0.6278\nEpoch 10/15\n1563/1563 [==============================] - 88s 56ms/step - loss: 1.1334 - accuracy: 0.6014 - val_loss: 1.1016 - val_accuracy: 0.6146\nEpoch 11/15\n1563/1563 [==============================] - 88s 56ms/step - loss: 1.1132 - accuracy: 0.6153 - val_loss: 1.2507 - val_accuracy: 0.5738\nEpoch 12/15\n1563/1563 [==============================] - 88s 56ms/step - loss: 1.1198 - accuracy: 0.6161 - val_loss: 1.0975 - val_accuracy: 0.6157\nEpoch 13/15\n1563/1563 [==============================] - 88s 56ms/step - loss: 1.0996 - accuracy: 0.6220 - val_loss: 1.0770 - val_accuracy: 0.6321\nEpoch 14/15\n1563/1563 [==============================] - 86s 55ms/step - loss: 1.0864 - accuracy: 0.6251 - val_loss: 1.0508 - val_accuracy: 0.6357\nEpoch 15/15\n1563/1563 [==============================] - 88s 56ms/step - loss: 1.0828 - accuracy: 0.6254 - val_loss: 1.0559 - val_accuracy: 0.6391\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f0aa0072d90>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "#gives us the probability for each of our classes\nmodel_1.predict(x_test)", "execution_count": 18, "outputs": [{"output_type": "execute_result", "execution_count": 18, "data": {"text/plain": "array([[6.8839043e-03, 1.1621309e-03, 1.4283001e-02, ..., 4.6756314e-03,\n        2.0176597e-02, 4.5924261e-03],\n       [2.8112024e-02, 7.5305223e-01, 1.7863354e-07, ..., 8.6407227e-12,\n        2.1845166e-01, 3.8393482e-04],\n       [1.6166897e-01, 4.6793941e-02, 2.5714761e-02, ..., 7.2043319e-03,\n        6.5821689e-01, 6.4712256e-02],\n       ...,\n       [8.0100481e-06, 2.4552202e-07, 4.3136179e-03, ..., 9.8891230e-03,\n        8.7811014e-07, 1.3435193e-06],\n       [1.2053891e-02, 7.9465158e-02, 3.6364313e-02, ..., 3.3052854e-02,\n        6.9035414e-05, 4.3065532e-04],\n       [1.7699205e-05, 1.2943922e-06, 1.1706100e-04, ..., 9.7006458e-01,\n        3.2697938e-09, 2.0350367e-06]], dtype=float32)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "#for a specific class we can do, this just applies the threshold for us to determine the class\n#model_1.predict_classes(x_test)\nimport numpy as np\nnp.argmax(model_1.predict(x_test), axis=-1)", "execution_count": 22, "outputs": [{"output_type": "execute_result", "execution_count": 22, "data": {"text/plain": "array([3, 1, 8, ..., 5, 4, 7])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "#if we want to evaluate convert y test back to a class rather than onehotencoding\nimport numpy as np\nnp.argmax(y_test, axis=1)\n#this gives our actual values\nfrom sklearn.metrics import accuracy_score\naccuracy_score(np.argmax(y_test, axis=1), np.argmax(model_1.predict(x_test), axis=-1))", "execution_count": 24, "outputs": [{"output_type": "execute_result", "execution_count": 24, "data": {"text/plain": "0.6391"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Exercise\nOur previous model had the structure:\n\nConv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n\n(with appropriate activation functions and dropouts)\n\n1. Build a more complicated model with the following pattern:\n- Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n\n- Use strides of 1 for all convolutional layers.\n\n2. How many parameters does your model have?  How does that compare to the previous model?\n\n3. Train it for 5 epochs.  What do you notice about the training time, loss and accuracy numbers (on both the training and validation sets)?\n\n5. Try different structures and run times, and see how accurate your model can be.\n"}, {"metadata": {}, "cell_type": "code", "source": "# Let's build a CNN using Keras' Sequential capabilities\n\nmodel_2 = Sequential()\n\nmodel_2.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:])) #strides at default 1x1\nmodel_2.add(Activation('relu'))\nmodel_2.add(Conv2D(32, (3, 3)))\nmodel_2.add(Activation('relu'))\nmodel_2.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2.add(Dropout(0.25))\n# adding another two convolutional layer\nmodel_2.add(Conv2D(64, (3, 3), padding='same'))\nmodel_2.add(Activation('relu'))\nmodel_2.add(Conv2D(64, (3, 3)))\nmodel_2.add(Activation('relu'))\nmodel_2.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2.add(Dropout(0.25))\n\nmodel_2.add(Flatten())\nmodel_2.add(Dense(512))\nmodel_2.add(Activation('relu'))\nmodel_2.add(Dropout(0.5))\nmodel_2.add(Dense(num_classes))\nmodel_2.add(Activation('softmax'))", "execution_count": 25, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## Check number of parameters\n\nmodel_2.summary()", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_2 (Conv2D)            (None, 32, 32, 32)        896       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 32, 32, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 30, 30, 32)        9248      \n_________________________________________________________________\nactivation_5 (Activation)    (None, 30, 30, 32)        0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 15, 15, 32)        0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 15, 15, 64)        18496     \n_________________________________________________________________\nactivation_6 (Activation)    (None, 15, 15, 64)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 13, 13, 64)        36928     \n_________________________________________________________________\nactivation_7 (Activation)    (None, 13, 13, 64)        0         \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 6, 6, 64)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 2304)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 512)               1180160   \n_________________________________________________________________\nactivation_8 (Activation)    (None, 512)               0         \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                5130      \n_________________________________________________________________\nactivation_9 (Activation)    (None, 10)                0         \n=================================================================\nTotal params: 1,250,858\nTrainable params: 1,250,858\nNon-trainable params: 0\n_________________________________________________________________\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# initiate RMSprop optimizer\nopt_2 = RMSprop(lr=0.0005)\n\n# Let's train the model using RMSprop\nmodel_2.compile(loss='categorical_crossentropy',\n              optimizer=opt_2,\n              metrics=['accuracy'])", "execution_count": 27, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "model_2.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=5,\n              validation_data=(x_test, y_test),\n              shuffle=True)", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "Epoch 1/5\n1563/1563 [==============================] - 543s 347ms/step - loss: 1.7949 - accuracy: 0.3374 - val_loss: 1.2572 - val_accuracy: 0.5548\nEpoch 2/5\n1563/1563 [==============================] - 529s 339ms/step - loss: 1.2455 - accuracy: 0.5572 - val_loss: 1.3926 - val_accuracy: 0.5010\nEpoch 3/5\n1563/1563 [==============================] - 521s 333ms/step - loss: 1.0586 - accuracy: 0.6269 - val_loss: 0.9694 - val_accuracy: 0.6675\nEpoch 4/5\n1563/1563 [==============================] - 513s 328ms/step - loss: 0.9561 - accuracy: 0.6655 - val_loss: 0.8974 - val_accuracy: 0.6875\nEpoch 5/5\n1563/1563 [==============================] - 538s 344ms/step - loss: 0.8911 - accuracy: 0.6943 - val_loss: 0.9280 - val_accuracy: 0.6859\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 28, "data": {"text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f0a08428460>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "---\n### Machine Learning Foundation (C) 2020 IBM Corporation"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.8", "language": "python"}, "language_info": {"name": "python", "version": "3.8.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}